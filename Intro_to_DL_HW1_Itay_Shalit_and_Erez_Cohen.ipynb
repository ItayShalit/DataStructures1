{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to DL - HW1 - Itay Shalit and Erez Cohen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItayShalit/DataStructures1/blob/main/Intro_to_DL_HW1_Itay_Shalit_and_Erez_Cohen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Itay Shalit 207435199 <br>\n",
        "Erez Cohen 208848531\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0sDIPl4fApV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.nn.utils import skip_init\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from sys import maxsize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from collections import OrderedDict\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "IN4DVHn0Ao3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1\n"
      ],
      "metadata": {
        "id": "MeIsRnK0hTFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "jlH7jA8fYokc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 5000\n",
        "TEST_SIZE = 1000\n",
        "\n",
        "gen = torch.Generator()\n",
        "gen.manual_seed(2022)\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, \n",
        "                                        transform = transforms.ToTensor())\n",
        "\n",
        "sample_train_idx = torch.randint(len(trainset), size=(TRAIN_SIZE,), generator = gen)\n",
        "\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root = './data', train = False,\n",
        "                                       download = True, transform = transforms.ToTensor())\n",
        "sample_test_idx = torch.randint(len(testset), size=(TEST_SIZE,), generator = gen)\n"
      ],
      "metadata": {
        "id": "bAj_PERXhjBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training an SVM Classifier "
      ],
      "metadata": {
        "id": "6Wu62x9vYaIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INTENSITY = 255\n",
        "\n",
        "train_x = trainset.data/MAX_INTENSITY\n",
        "train_y = trainset.targets\n",
        "\n",
        "test_x = testset.data/MAX_INTENSITY\n",
        "test_y = testset.targets\n",
        "\n",
        "train_x = train_x.reshape(train_x.shape[0], -1)\n",
        "train_x = train_x[sample_train_idx,:]\n",
        "train_y = np.array(train_y)[sample_train_idx]\n",
        "\n",
        "test_x = test_x.reshape(test_x.shape[0], -1)\n",
        "test_x = test_x[sample_test_idx,:]\n",
        "test_y = np.array(test_y)[sample_test_idx]"
      ],
      "metadata": {
        "id": "2uSXgRVlT2ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Msm3GyssaTyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid1 = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.001],'kernel': ['rbf']}\n",
        "param_grid2 = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.001],'kernel': ['linear']}\n",
        "\n",
        "grid1 = GridSearchCV(SVC(),param_grid1, refit=True, verbose=2, n_jobs = -1)\n",
        "grid2 = GridSearchCV(SVC(),param_grid2, refit=True, verbose=2, n_jobs = -1)\n",
        "\n",
        "grid1.fit(train_x, train_y)\n",
        "print(\"best parameters for grid1: \", grid1.best_params_)\n",
        "\n",
        "grid2.fit(train_x, train_y)\n",
        "print(\"best parameters for grid2: \", grid2.best_params_)"
      ],
      "metadata": {
        "id": "RkneHGP-XyZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the Models"
      ],
      "metadata": {
        "id": "DbwBiEQyaY0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The follwing are the parameters found to be best:\n",
        "params1 = {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
        "params2 = {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}\n",
        "\n",
        "rbf_model = SVC(C = params1[\"C\"], kernel = \"rbf\", gamma = params1[\"gamma\"])\n",
        "linear_model = SVC(C = params2[\"C\"], kernel = \"linear\", gamma = params2[\"gamma\"])\n",
        "\n",
        "rbf_model.fit(train_x, train_y)\n",
        "linear_model.fit(train_x, train_y)\n",
        "\n",
        "test_pred_rbf = rbf_model.predict(test_x)\n",
        "test_pred_linear = linear_model.predict(test_x)\n",
        "\n",
        "print(f\"\"\"accuracy score for linear model is: {accuracy_score(test_y, test_pred_linear)} \\n \n",
        "        accuracy score for rbf model is: {accuracy_score(test_y, test_pred_rbf)}\"\"\")"
      ],
      "metadata": {
        "id": "sFv5zFW4P1tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Networks Preliminaries"
      ],
      "metadata": {
        "id": "EYY9hZ6zt5_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['train_accuracy', 'test_accuracy', 'train_loss', 'test_loss']\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, device, print_progress = False):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if print_progress&(batch % 100 == 0):\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, device, return_results = False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    \n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    if return_results:\n",
        "      return test_loss, correct\n",
        "\n",
        "def train_and_return_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device):\n",
        "  train_accuracy_vals, train_loss_vals, test_accuracy_vals, test_loss_vals = [], [], [], []\n",
        "  for t in range(epochs):\n",
        "    train_loop(trainloader, model, loss_fn, optimizer, device)\n",
        "    train_loss, train_accuracy = test_loop(trainloader, model, loss_fn, device, True)\n",
        "    test_loss, test_accuracy = test_loop(testloader, model, loss_fn, device, True)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "  return train_accuracy_vals, train_loss_vals, test_accuracy_vals, test_loss_vals\n",
        "\n",
        "def plot_results(models_results):\n",
        "  figure = plt.figure(figsize=(18, 6))\n",
        "  ax1 = figure.add_subplot(1,2,1)\n",
        "  ax2 = figure.add_subplot(1,2,2)\n",
        "  ax1 = figure.get_axes()[0]\n",
        "  ax2 = figure.get_axes()[1]\n",
        "  x_axis = [i for i in range(epochs)]\n",
        "  for results, labels in models_results:\n",
        "    ax1.plot(x_axis, results[0], label=labels[0])\n",
        "    ax1.plot(x_axis, results[2], label=labels[2])\n",
        "    ax2.plot(x_axis, results[1], label=labels[1])\n",
        "    ax2.plot(x_axis, results[3], label=labels[3])\n",
        "  ax1.legend()\n",
        "  ax2.legend()\n",
        "  figure.show()\n",
        "\n",
        "def train_and_plot_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device):\n",
        "  results = train_and_return_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device)\n",
        "  print(\"accuracy: \", results[2][-1], \" loss: \", results[3][-1])\n",
        "  plot_results([(results, labels)])"
      ],
      "metadata": {
        "id": "q9CIfOrqDuN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Initialization and Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "xxM6tLtPPrVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "trainsubset = torch.utils.data.Subset(trainset, sample_train_idx)\n",
        "testsubset = torch.utils.data.Subset(testset, sample_test_idx)\n",
        "trainloader = torch.utils.data.DataLoader(trainsubset, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testsubset, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "L7nlnpJWGmqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_vals = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "init_std_vals = [1e-3, 1e-1, 1]\n",
        "momentum_param_vals = [0.9, 0.95, 0.99] \n",
        "\n",
        "best_configuration = {'learning_rate':None, 'init_std': None, 'momentum_param': None}\n",
        "\n",
        "def init_weights_normal(model, init_std):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "      torch.nn.init.normal_(m.weight, mean=0.0, std=init_std)\n",
        "      if m.bias is not None:\n",
        "        torch.nn.init.normal_(m.bias, mean=0.0, std=init_std)\n",
        "\n",
        "def hyper_parameters_grid_search(model, loss_func):\n",
        "  best_loss = maxsize\n",
        "  for learning_rate in learning_rate_vals:\n",
        "    for init_std in init_std_vals:\n",
        "      for momentum_param in momentum_param_vals:\n",
        "        init_weights_normal(model, init_std)\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum_param)\n",
        "        for t in range(epochs):\n",
        "          train_loop(trainloader, model, loss_func, optimizer, device)\n",
        "        loss, accuracy = test_loop(testloader, model, loss_func, device, True)\n",
        "        print(\"learning rate: \", learning_rate, \"init_std: \", init_std , \"momentum_param: \", momentum_param, \"loss: \", loss)\n",
        "        if loss<best_loss:\n",
        "          best_loss = loss\n",
        "          best_configuration[\"learning_rate\"] = learning_rate\n",
        "          best_configuration[\"momentum_param\"] = momentum_param\n",
        "          best_configuration[\"init_std\"] = init_std"
      ],
      "metadata": {
        "id": "SUb9NCj6Puj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "### Baseline"
      ],
      "metadata": {
        "id": "xZ-Gg2aVC864"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        input_layer = nn.Linear(3072, 256)\n",
        "        hidden_layer = nn.Linear(256, 10)\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            input_layer,\n",
        "            nn.ReLU(),\n",
        "            hidden_layer,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "qaxbyq-jC_Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model = FFNeuralNetwork()\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "fHSqLjHzDlu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting training procces with best configuration"
      ],
      "metadata": {
        "id": "AeDagYK6QEtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10 # for sanity check only\n",
        "hyper_parameters_grid_search(model, loss_fn)\n",
        "init_weights_normal(model, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "Lq9ftOAqESLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimization"
      ],
      "metadata": {
        "id": "rcOdSBN6DbUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(model, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "train_and_plot_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "SPESFoySDdf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization"
      ],
      "metadata": {
        "id": "LFDvnT4ZFRGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.init.xavier_normal_(model.linear_relu_stack[0].weight)\n",
        "torch.nn.init.xavier_normal_(model.linear_relu_stack[2].weight)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "PMQzG7K0FR8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weight Decay"
      ],
      "metadata": {
        "id": "Y6P0E9jeovyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(model, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"], weight_decay=1e-4)\n",
        "train_and_plot_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "bFjDCe3rokwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropout"
      ],
      "metadata": {
        "id": "y0SipZn3oy13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNeuralNetworkDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNeuralNetworkDropout, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        input_layer = nn.Linear(3072, 256)\n",
        "        hidden_layer = nn.Linear(256, 10)\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            input_layer,\n",
        "            nn.Dropout(0.25),\n",
        "            nn.ReLU(),\n",
        "            hidden_layer\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "pVb9jM4-qjDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FFNeuralNetworkDropout().to(device)\n",
        "init_weights_normal(model, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "B8XOE8Pno1j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCA Whitening"
      ],
      "metadata": {
        "id": "2bklyyplsrsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNeuralNetworkPCA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNeuralNetworkPCA, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        input_layer = nn.Linear(300, 256)\n",
        "        hidden_layer = nn.Linear(256, 10)\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            input_layer,\n",
        "            nn.ReLU(),\n",
        "            hidden_layer,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "pca = PCA(n_components=300, whiten=True, random_state=10)\n",
        "train_x_pca = pca.fit_transform(train_x)\n",
        "test_x_pca = pca.fit_transform(test_x)\n",
        "tensor_trainx = torch.Tensor(train_x_pca)\n",
        "tensor_trainy = torch.Tensor(train_y)\n",
        "tensor_trainy = tensor_trainy.type(torch.LongTensor)\n",
        "tensor_testx = torch.Tensor(test_x_pca)\n",
        "tensor_testy = torch.Tensor(test_y)\n",
        "tensor_testy = tensor_testy.type(torch.LongTensor)\n",
        "\n",
        "trainset_pca = torch.utils.data.TensorDataset(tensor_trainx, tensor_trainy)\n",
        "testset_pca = torch.utils.data.TensorDataset(tensor_testx, tensor_testy)\n",
        "\n",
        "trainloader_pca = torch.utils.data.DataLoader(trainset_pca, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)\n",
        "testloader_pca = torch.utils.data.DataLoader(testset_pca, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)\n",
        "\n",
        "model = FFNeuralNetworkPCA().to(device)\n",
        "\n",
        "init_weights_normal(model, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader_pca, testloader_pca, model, loss_fn, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "RAlFO0Assq_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Network Width"
      ],
      "metadata": {
        "id": "SLeHCfRTumgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NNClassGeneratorByWidth(width):\n",
        "  class FFNeuralNetwork(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(FFNeuralNetwork, self).__init__()\n",
        "          self.flatten = nn.Flatten()\n",
        "          input_layer = nn.Linear(3072, width)\n",
        "          hidden_layer = nn.Linear(width, 10)\n",
        "          self.linear_relu_stack = nn.Sequential(\n",
        "              input_layer,\n",
        "              nn.ReLU(),\n",
        "              hidden_layer,\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.flatten(x)\n",
        "          logits = self.linear_relu_stack(x)\n",
        "          return logits\n",
        "  return FFNeuralNetwork\n",
        "\n",
        "width_vals = [6, 10, 12]\n",
        "models_results = []\n",
        "for width in width_vals:\n",
        "  NNClass = NNClassGeneratorByWidth(2**width)\n",
        "  model = NNClass().to(device)\n",
        "  init_weights_normal(model, best_configuration[\"init_std\"])\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "  label_addition = f' width 2^{width}'\n",
        "  curr_labels = [lbl + label_addition for lbl in labels]\n",
        "  models_results.append((train_and_return_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device), curr_labels))\n",
        "\n",
        "plot_results(models_results)\n",
        "  "
      ],
      "metadata": {
        "id": "E9uXiuR-vBuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Network Depth"
      ],
      "metadata": {
        "id": "tUM23L-jvDyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NNClassGeneratorByDepth(depth):\n",
        "  class FFNeuralNetwork(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(FFNeuralNetwork, self).__init__()\n",
        "          self.flatten = nn.Flatten()\n",
        "          sequence = OrderedDict()\n",
        "          sequence['lin1'] = nn.Linear(3072, 64)\n",
        "          for i in range(depth-1):\n",
        "            sequence[f'rel{i+1}'] = nn.ReLU()\n",
        "            sequence[f'lin{i+2}'] = nn.Linear(64, 64)\n",
        "          sequence[f'rel{depth}'] = nn.ReLU()\n",
        "          sequence[f'rel{depth + 1}'] = nn.Linear(64, 10)\n",
        "          self.linear_relu_stack = nn.Sequential(sequence)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.flatten(x)\n",
        "          logits = self.linear_relu_stack(x)\n",
        "          return logits\n",
        "  return FFNeuralNetwork"
      ],
      "metadata": {
        "id": "APai1Oy1vFCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth_vals = [6, 10, 12]\n",
        "models_results = []\n",
        "for depth in depth_vals:\n",
        "  NNClass = NNClassGeneratorByDepth(depth)\n",
        "  model = NNClass().to(device)\n",
        "  for i in range(len(model.linear_relu_stack)):\n",
        "    if i%2 == 0:\n",
        "      torch.nn.init.normal_(model.linear_relu_stack[i].weight, mean=0.0, std=best_configuration['init_std'])\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "  label_addition = f' depth {depth}'\n",
        "  curr_labels = [lbl + label_addition for lbl in labels]\n",
        "  models_results.append((train_and_return_results(trainloader, testloader, model, loss_fn, optimizer, epochs, device), curr_labels))\n",
        "\n",
        "plot_results(models_results)"
      ],
      "metadata": {
        "id": "nlxOor-u25Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3\n"
      ],
      "metadata": {
        "id": "_Hc8K8rSv8sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3),\n",
        "            # nn.Dropout(0.15), # uncomment this line to enable dropout\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(64, 16, 3),\n",
        "            # nn.Dropout(0.3), # uncomment this line to enable dropout\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(784, 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "conv_net = ConvNet().to(device)"
      ],
      "metadata": {
        "id": "O78blZA9v9gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline"
      ],
      "metadata": {
        "id": "qM1lIb1gxGix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after an initial search with the values above, we narrowed it down to the these options\n",
        "learning_rate_vals = [0.01, 0.005]\n",
        "init_std_vals = [0.1]\n",
        "momentum_param_vals = [0.9, 0.95]\n",
        "epochs = 5 # sanity check only\n",
        "hyper_parameters_grid_search(model=conv_net, loss_func=conv_net.loss)\n",
        "print(\"best configuration:\", best_configuration)"
      ],
      "metadata": {
        "id": "sSGe98xwwNEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot results of best configuration\n",
        "init_weights_normal(conv_net, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(conv_net.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader, testloader, conv_net, conv_net.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "5qIB9PfBwVCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optimization"
      ],
      "metadata": {
        "id": "jFRSN7uCxNTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(conv_net, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.Adam(conv_net.parameters()) # note we are using default lr of 0.001\n",
        "train_and_plot_results(trainloader, testloader, conv_net, conv_net.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "-IybO2MCxMw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initialization"
      ],
      "metadata": {
        "id": "fEbOBNO6xzgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in conv_net.modules():\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "    torch.nn.init.xavier_normal_(m.weight)\n",
        "    torch.nn.init.zeros_(m.bias)\n",
        "optimizer = torch.optim.SGD(conv_net.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader, testloader, conv_net, conv_net.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "fsTSOKY0xRCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Weight Decay Regularization"
      ],
      "metadata": {
        "id": "RuKpMaTEx8i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(conv_net, best_configuration[\"init_std\"])\n",
        "# 1e-5 is best empirically out of (1e-3, 1e-4, 1e-5) although all are similar. 1e-2 takes too long to fit\n",
        "optimizer = torch.optim.SGD(conv_net.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"], weight_decay=1e-5)\n",
        "train_and_plot_results(trainloader, testloader, conv_net, conv_net.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "ia8UstBQxSiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dropout"
      ],
      "metadata": {
        "id": "Gge8DbPpyBQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the following result is from 30 epochs, 0.1 and 0.2 dropout on the first and second layer respectively\n",
        "init_weights_normal(conv_net, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(conv_net.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader, testloader, conv_net, conv_net.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "YCdiHnh7xUjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 50 epochs and yet higher dropout - 0.15 and 0.3\n",
        "# epochs = 50\n",
        "init_weights_normal(conv_net, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(conv_net.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader, testloader, conv_net, conv_net.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "_EFNFDK2xXS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing"
      ],
      "metadata": {
        "id": "mjge-wYhyKH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetPCA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNetPCA, self).__init__()\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3), # for multi-channel PCA, set to 3 input channels\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Conv2d(64, 16, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(144, 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "conv_net_pca = ConvNetPCA().to(device)"
      ],
      "metadata": {
        "id": "dXXHLGlZxYw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INTENSITY = 255\n",
        "train_x = trainset.data/MAX_INTENSITY\n",
        "train_x = train_x[sample_train_idx,:].transpose((3, 0, 1, 2)).reshape(3, 5000, -1)\n",
        "train_y = np.array(trainset.targets)[sample_train_idx]\n",
        "\n",
        "test_x = testset.data/MAX_INTENSITY\n",
        "test_x = test_x[sample_test_idx,:].transpose((3, 0, 1, 2)).reshape(3, 1000, -1)\n",
        "test_y = np.array(testset.targets)[sample_test_idx]\n",
        "\n",
        "pca = PCA(n_components=225, whiten=True) # see here why 225 components: https://towardsdatascience.com/integration-of-dimension-reduction-methods-and-neural-network-for-image-classification-96281963fe24\n",
        "train_x_pca = np.ndarray((3, 5000, 15, 15))\n",
        "test_x_pca = np.ndarray((3, 1000, 15, 15))\n",
        "for i in range(3):\n",
        "  train_x_pca[i] = pca.fit_transform(train_x[i]).reshape(train_x[i].shape[0], 15, 15)\n",
        "  test_x_pca[i] = pca.fit_transform(test_x[i]).reshape(test_x[i].shape[0], 15, 15)\n",
        "tensor_trainx = torch.Tensor(train_x_pca.transpose((1, 0, 2, 3)))\n",
        "tensor_trainy = torch.tensor(train_y)\n",
        "tensor_testx = torch.Tensor(test_x_pca.transpose((1, 0, 2, 3)))\n",
        "tensor_testy = torch.tensor(test_y)\n",
        "\n",
        "trainset_pca = torch.utils.data.TensorDataset(tensor_trainx,tensor_trainy)\n",
        "testset_pca = torch.utils.data.TensorDataset(tensor_testx,tensor_testy)\n",
        "trainloader_pca = torch.utils.data.DataLoader(trainset_pca, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)\n",
        "testloader_pca = torch.utils.data.DataLoader(testset_pca, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)"
      ],
      "metadata": {
        "id": "bcVeKzPfxaXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(conv_net_pca, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(conv_net_pca.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader_pca, testloader_pca, conv_net_pca, conv_net_pca.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "dofNF9PaxdG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flat PCA attempt\n",
        "MAX_INTENSITY = 255\n",
        "train_x = trainset.data/MAX_INTENSITY\n",
        "train_x = train_x[sample_train_idx,:].reshape(len(sample_train_idx), -1)\n",
        "train_y = np.array(trainset.targets)[sample_train_idx]\n",
        "\n",
        "test_x = testset.data/MAX_INTENSITY\n",
        "test_x = test_x[sample_test_idx,:].reshape(len(sample_test_idx), -1)\n",
        "test_y = np.array(testset.targets)[sample_test_idx]\n",
        "\n",
        "pca = PCA(n_components=225, whiten=True)\n",
        "train_x_pca = pca.fit_transform(train_x).reshape(train_x.shape[0], 15, 15)\n",
        "test_x_pca = pca.fit_transform(test_x).reshape(test_x.shape[0], 15, 15)\n",
        "\n",
        "tensor_trainx = torch.Tensor(train_x_pca).reshape(train_x.shape[0], 1, 15, 15)\n",
        "tensor_trainy = torch.tensor(train_y)\n",
        "tensor_testx = torch.Tensor(test_x_pca).reshape(test_x.shape[0], 1, 15, 15)\n",
        "tensor_testy = torch.tensor(test_y)\n",
        "\n",
        "trainset_pca = torch.utils.data.TensorDataset(tensor_trainx,tensor_trainy)\n",
        "testset_pca = torch.utils.data.TensorDataset(tensor_testx,tensor_testy)\n",
        "trainloader_pca = torch.utils.data.DataLoader(trainset_pca, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)\n",
        "testloader_pca = torch.utils.data.DataLoader(testset_pca, batch_size = batch_size,\n",
        "                                          num_workers=2, shuffle = True)"
      ],
      "metadata": {
        "id": "rkXoglX6xexh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(conv_net_pca, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(conv_net_pca.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "train_and_plot_results(trainloader_pca, testloader_pca, conv_net_pca, conv_net_pca.loss, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "N0qVkFNrxey7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Network Width"
      ],
      "metadata": {
        "id": "1R9wI3rryQAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetWide(ConvNet):\n",
        "    def __init__(self):\n",
        "        super(ConvNetWide, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(3, 256, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(256, 64, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "class ConvNetWider(ConvNet):\n",
        "    def __init__(self):\n",
        "        super(ConvNetWider, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(3, 512, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(512, 256, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(12544, 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "conv_net_wide = ConvNetWide().to(device)\n",
        "conv_net_wider = ConvNetWider().to(device)"
      ],
      "metadata": {
        "id": "803-kxLDxe3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(conv_net_wide, best_configuration[\"init_std\"])\n",
        "init_weights_normal(conv_net_wider, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.SGD(conv_net_wide.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "wide_results = train_and_return_results(trainloader, testloader, conv_net_wide, conv_net_wide.loss, optimizer, epochs, device)\n",
        "optimizer = torch.optim.SGD(conv_net_wider.parameters(), lr=best_configuration[\"learning_rate\"], momentum=best_configuration[\"momentum_param\"])\n",
        "wider_results = train_and_return_results(trainloader, testloader, conv_net_wider, conv_net_wider.loss, optimizer, epochs, device)\n",
        "\n",
        "wide_labels = ['train accuracy wide', 'test accuracy wide', 'train loss wide', 'test loss wide']\n",
        "wider_labels = ['train accuracy WIDER', 'test accuracy WIDER', 'train loss WIDER', 'test loss WIDER']\n",
        "\n",
        "plot_results([(wide_results, wide_labels), (wider_results, wider_labels)])"
      ],
      "metadata": {
        "id": "ipLyUfI4xk3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Network Depth"
      ],
      "metadata": {
        "id": "hJhnXWv7yVOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetDeep(ConvNet):\n",
        "    def __init__(self, depth, linear_size):\n",
        "        super(ConvNetDeep, self).__init__()\n",
        "        layers = [nn.Conv2d(3, 2**(depth + 4), 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)]\n",
        "        current_channels = 2**(depth + 4)\n",
        "        for i in range(depth - 1):\n",
        "          layers += [nn.Conv2d(current_channels, 2**(depth + 3 - i), 3),\n",
        "            nn.ReLU()]\n",
        "          if i == int(depth / 2) - 1: # only add one additional max pooling layer because dimensions get ever smaller\n",
        "            layers.append(nn.MaxPool2d(2, stride=2, ceil_mode=True))\n",
        "          current_channels = 2**(depth + 3 - i)\n",
        "        layers += [nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "                   nn.Flatten(),\n",
        "                   nn.Linear(linear_size, 10)]\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "conv_net_3 = ConvNetDeep(3, 288).to(device)\n",
        "conv_net_4 = ConvNetDeep(4, 128).to(device)\n",
        "conv_net_5 = ConvNetDeep(5, 32).to(device)"
      ],
      "metadata": {
        "id": "Sar_bplBxme9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(conv_net_3, best_configuration[\"init_std\"])\n",
        "init_weights_normal(conv_net_4, best_configuration[\"init_std\"])\n",
        "init_weights_normal(conv_net_5, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.Adam(conv_net_3.parameters())\n",
        "results_3 = train_and_return_results(trainloader, testloader, conv_net_3, conv_net_3.loss, optimizer, epochs, device)\n",
        "optimizer = torch.optim.Adam(conv_net_4.parameters())\n",
        "results_4 = train_and_return_results(trainloader, testloader, conv_net_4, conv_net_4.loss, optimizer, epochs, device)\n",
        "optimizer = torch.optim.Adam(conv_net_5.parameters())\n",
        "results_5 = train_and_return_results(trainloader, testloader, conv_net_5, conv_net_5.loss, optimizer, epochs, device)\n",
        "\n",
        "labels = ['train accuracy', 'test accuracy', 'train loss', 'test loss']\n",
        "labels_3 = [lbl + ' 3' for lbl in labels]\n",
        "labels_4 = [lbl + ' 4' for lbl in labels]\n",
        "labels_5 = [lbl + ' 5' for lbl in labels]\n",
        "\n",
        "plot_results([(results_3, labels_3), (results_4, labels_4), (results_5, labels_5)])"
      ],
      "metadata": {
        "id": "3n9ZR7D1xoi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Residual Connections"
      ],
      "metadata": {
        "id": "LHRr3KQY7LLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapted from: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/CNN_architectures/pytorch_resnet.py\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, in_channels, first_out_channels, iters = 1):\n",
        "    super(ResBlock, self).__init__()\n",
        "    layers = [nn.Conv2d(in_channels, first_out_channels, kernel_size=1)]\n",
        "    current_channels = first_out_channels\n",
        "    self.relu = nn.ReLU()\n",
        "    for _i in range(iters):\n",
        "      layers += [self.relu, nn.Conv2d(current_channels, 2*current_channels, kernel_size=1)]\n",
        "      current_channels *= 2\n",
        "    self.block = nn.Sequential(*layers)\n",
        "    self.identity_pad = nn.Conv2d(in_channels, current_channels, kernel_size=1, bias=False) if in_channels != current_channels else None\n",
        "    self.max_pool = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "    x = self.block(x)\n",
        "    if self.identity_pad is not None:\n",
        "      identity = self.identity_pad(identity)\n",
        "    x += identity\n",
        "    x = self.relu(x)\n",
        "    x = self.max_pool(x)\n",
        "    return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, depth):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.res_blocks = []\n",
        "        curr_channels = 64\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.first_block = [\n",
        "            nn.Conv2d(3, curr_channels, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        ]\n",
        "        if (depth % 2) == 0:\n",
        "          self.res_blocks.append(ResBlock(64, 64, 2))\n",
        "          depth -= 3\n",
        "          curr_channels = 256\n",
        "        for _i in range(int((depth - 1) / 2)):\n",
        "          self.res_blocks.append(ResBlock(curr_channels, 128))\n",
        "          curr_channels = 256\n",
        "        self.classifier = nn.Sequential(\n",
        "            *self.first_block,\n",
        "            *self.res_blocks,\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(int(16384 / (4**(len(self.res_blocks) - 1))), 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "res_net_3 = ResNet(3).to(device)\n",
        "res_net_4 = ResNet(4).to(device)\n",
        "res_net_5 = ResNet(5).to(device)"
      ],
      "metadata": {
        "id": "ntpHHizn7Peu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_weights_normal(res_net_3, best_configuration[\"init_std\"])\n",
        "init_weights_normal(res_net_4, best_configuration[\"init_std\"])\n",
        "init_weights_normal(res_net_5, best_configuration[\"init_std\"])\n",
        "optimizer = torch.optim.Adam(res_net_3.parameters())\n",
        "results_3 = train_and_return_results(trainloader, testloader, res_net_3, res_net_3.loss, optimizer, epochs, device)\n",
        "optimizer = torch.optim.Adam(res_net_4.parameters())\n",
        "results_4 = train_and_return_results(trainloader, testloader, res_net_4, res_net_4.loss, optimizer, epochs, device)\n",
        "optimizer = torch.optim.Adam(res_net_5.parameters())\n",
        "results_5 = train_and_return_results(trainloader, testloader, res_net_5, res_net_5.loss, optimizer, epochs, device)\n",
        "\n",
        "labels = ['train accuracy', 'train loss', 'test accuracy', 'test loss']\n",
        "labels_3 = [lbl + ' 3' for lbl in labels]\n",
        "labels_4 = [lbl + ' 4' for lbl in labels]\n",
        "labels_5 = [lbl + ' 5' for lbl in labels]\n",
        "\n",
        "plot_results([(results_3, labels_3), (results_4, labels_4), (results_5, labels_5)])"
      ],
      "metadata": {
        "id": "uyZbINamcIAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4"
      ],
      "metadata": {
        "id": "5CKVdlOU_wHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Recurrent Neural Network - Basic Architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "TGuMY1vbJ15E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diclosure - For this part, we used the skeleton of an official tutorial by Pytorch."
      ],
      "metadata": {
        "id": "H7CHVdslAMoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "# Previous to running this cell, user should load the data to local memory of the notebook\n",
        "# from https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
        "print(findFiles('data/names/*'))\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('lusrski'))\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)"
      ],
      "metadata": {
        "id": "kpplBJEX_x2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split to train and test sets\n",
        "\n",
        "trainset = {} \n",
        "testset = {}\n",
        "SEED = 10\n",
        "\n",
        "for category in all_categories:\n",
        "  lines = category_lines[category]\n",
        "  random.Random(SEED).shuffle(lines)\n",
        "  seperator = int(len(lines)*0.2)\n",
        "  testset[category] = lines[:seperator]\n",
        "  trainset[category] = lines[seperator:]"
      ],
      "metadata": {
        "id": "X6EczqxMKu6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "ahUggOv7Ayml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import Hardtanh\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # Building your RNN\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, input_dim)\n",
        "        # batch_dim = number of samples per batch\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # This is part of truncated backpropagation through time (BPTT)\n",
        "\n",
        "        x = torch.unsqueeze(x, 1)\n",
        "        # out, hn = self.rnn(x, hidden.detach()) We need to detach the hidden state to prevent exploding/vanishing gradients, but I did not do that because of the given task.\n",
        "        out, hn = self.rnn(x, hidden)\n",
        "        out = self.fc(out[:, -1, :]) \n",
        "        out = self.softmax(out)\n",
        "        return out, hn\n",
        "\n",
        "    def initHidden(self):\n",
        "        #  return torch.zeros(1, self.hidden_dim)\n",
        "         return torch.zeros(self.layer_dim, 1, self.hidden_dim).requires_grad_()"
      ],
      "metadata": {
        "id": "pUiZJJynCc6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(trainset[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n"
      ],
      "metadata": {
        "id": "Y8hm1nyQI4Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, category_tensor, line_tensor, criterion):\n",
        "    hidden = model.initHidden()\n",
        "    model.zero_grad()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = model(line_tensor[i], hidden)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    # Add parameters' gradients to their values, multiplied by learning rate\n",
        "    for p in model.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return output, loss.item()\n",
        "\n",
        "def calculate_test_loss(model, test_set, criterion):\n",
        "  counter = 0\n",
        "  loss = 0\n",
        "  with torch.no_grad(): \n",
        "    for category in test_set:\n",
        "      for line in test_set[category]:\n",
        "        counter += 1\n",
        "        hidden = model.initHidden()\n",
        "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "        line_tensor = lineToTensor(line)\n",
        "        for i in range(line_tensor.size()[0]):\n",
        "          output, hidden = model(line_tensor[i], hidden)\n",
        "        loss += criterion(output, category_tensor)\n",
        "  return loss/counter\n",
        "\n",
        "def calculate_gradient_l2_norm(model):\n",
        "  mean_l2_norm = 0\n",
        "  grad_num = 0\n",
        "  for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "    grad_num += len(p)\n",
        "  for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "    mean_l2_norm += (p.grad.data.norm(2).item())*(len(p)/grad_num)\n",
        "  return mean_l2_norm"
      ],
      "metadata": {
        "id": "QFj4UYt-JczY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 100\n",
        "layer_dim = 3\n",
        "rnn = RNNModel(n_letters, hidden_dim, layer_dim, n_categories)\n",
        "hidden = torch.zeros(layer_dim, 1, hidden_dim).requires_grad_()\n",
        "learning_rate = 0.005 \n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "A6U3Nm2xEDCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "metadata": {
        "id": "mxCmpvJyOmtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 200000\n",
        "print_every = 5000\n",
        "plot_every = 4000\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "param_l2_norm = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train(rnn, category_tensor, line_tensor, criterion)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '' if guess == category else ' (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        train_losses.append(current_loss / iter)\n",
        "        test_losses.append(calculate_test_loss(rnn, testset, criterion))\n",
        "        param_l2_norm.append(calculate_gradient_l2_norm(rnn))\n",
        "        "
      ],
      "metadata": {
        "id": "3S4xu4vWRfYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label = 'train loss')\n",
        "plt.plot(test_losses, label = 'test loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "hpkaLBPgR5Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(param_l2_norm, label = 'gradients l2-norm')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Xssb1YKHX3o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Recurrent Neural Network - LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "QbCepEuJN3Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # Building your RNN\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, input_dim)\n",
        "        # batch_dim = number of samples per batch\n",
        "\n",
        "        self.rnn = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, num_layers = layer_dim, batch_first=True)\n",
        "        # self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # # Initialize hidden state with zeros\n",
        "        # # (layer_dim, batch_size, hidden_dim)\n",
        "        # h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
        "        # This is part of truncated backpropagation through time (BPTT)\n",
        "        \n",
        "        x = torch.unsqueeze(x, 1)\n",
        "\n",
        "        # sequence length, batch size, input size\n",
        "        out, hn = self.rnn(x, hidden)\n",
        "\n",
        "        # Index hidden state of last time step\n",
        "        # out.size() --> 100, 28, 10\n",
        "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
        "        out = self.fc(out[:, -1, :]) \n",
        "        # out.size() --> 100, 10\n",
        "        out = self.softmax(out)\n",
        "        return out, hn\n",
        "\n",
        "    def initHidden(self):\n",
        "        #  return torch.zeros(1, self.hidden_dim)\n",
        "         return [torch.zeros(self.layer_dim, 1, self.hidden_dim).requires_grad_(), torch.zeros(self.layer_dim, 1, self.hidden_dim).requires_grad_()]"
      ],
      "metadata": {
        "id": "PGspAsOscEca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 100\n",
        "layer_dim = 3\n",
        "lstm = LSTMModel(n_letters, hidden_dim, layer_dim, n_categories)\n",
        "# hidden = torch.zeros(layer_dim, 1, hidden_dim).requires_grad_()\n",
        "learning_rate = 0.005 \n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "kWxCyZipO5hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 200000\n",
        "print_every = 5000\n",
        "plot_every = 4000\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "gradient_l2_norm = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output, loss = train(lstm, category_tensor, line_tensor, criterion)\n",
        "    current_loss += loss\n",
        "\n",
        "    # Print iter number, loss, name and guess\n",
        "    if iter % print_every == 0:\n",
        "        guess, guess_i = categoryFromOutput(output)\n",
        "        correct = '' if guess == category else ' (%s)' % category\n",
        "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        train_losses.append(current_loss / iter)\n",
        "        test_losses.append(calculate_test_loss(lstm, testset, criterion))\n",
        "        param_l2_norm.append(calculate_gradient_l2_norm(lstm))"
      ],
      "metadata": {
        "id": "KSznJF3SSk5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label = 'train loss')\n",
        "plt.plot(test_losses, label = 'test loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "yigYsb3WO3vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(param_l2_norm, label = 'gradients l2-norm')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "YMoQmEoIdOY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}